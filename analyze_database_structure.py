#!/usr/bin/env python3
"""
æ•°è„‰é“¶è¡Œæ•°æ®åº“ç»“æ„åˆ†æè„šæœ¬

è¯¥è„šæœ¬ç”¨äºï¼š
1. æå–æ•°æ®åº“çš„è¯¦ç»†ç»“æ„ä¿¡æ¯
2. åˆ†æè¡¨ç»“æ„çš„åˆç†æ€§
3. æ£€æŸ¥ç´¢å¼•ä¼˜åŒ–æƒ…å†µ
4. åˆ†æå¤–é”®å…³ç³»
5. ç”Ÿæˆç»“æ„åˆ†ææŠ¥å‘Š
"""

import os
import sys
import json
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent / "corebank-backend"
sys.path.insert(0, str(project_root))

@dataclass
class ColumnInfo:
    """åˆ—ä¿¡æ¯æ•°æ®ç±»"""
    column_name: str
    data_type: str
    is_nullable: bool
    column_default: Optional[str]
    character_maximum_length: Optional[int]
    numeric_precision: Optional[int]
    numeric_scale: Optional[int]
    is_primary_key: bool = False
    is_foreign_key: bool = False
    foreign_table: Optional[str] = None
    foreign_column: Optional[str] = None

@dataclass
class IndexInfo:
    """ç´¢å¼•ä¿¡æ¯æ•°æ®ç±»"""
    index_name: str
    table_name: str
    column_names: List[str]
    is_unique: bool
    is_primary: bool
    index_type: str

@dataclass
class TableInfo:
    """è¡¨ä¿¡æ¯æ•°æ®ç±»"""
    table_name: str
    columns: List[ColumnInfo]
    indexes: List[IndexInfo]
    row_count: int
    table_size: str
    constraints: List[Dict[str, Any]]

class DatabaseAnalyzer:
    """æ•°æ®åº“ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.connection = None
        self.cursor = None
        self.analysis_results = {}
        
    def connect_to_database(self) -> bool:
        """è¿æ¥åˆ°æ•°æ®åº“"""
        try:
            # ä»ç¯å¢ƒå˜é‡æˆ–.envæ–‡ä»¶è¯»å–æ•°æ®åº“é…ç½®
            # å¦‚æœåœ¨å®¹å™¨å¤–è¿è¡Œï¼Œä½¿ç”¨localhostè€Œä¸æ˜¯postgres
            host = os.getenv('POSTGRES_HOST', 'localhost')
            if host == 'postgres':
                host = 'localhost'  # å®¹å™¨å¤–è®¿é—®ä½¿ç”¨localhost

            db_config = {
                'host': host,
                'port': int(os.getenv('POSTGRES_PORT', 5432)),
                'database': os.getenv('POSTGRES_DB', 'corebank'),
                'user': os.getenv('POSTGRES_USER', 'corebank_user'),
                'password': os.getenv('POSTGRES_PASSWORD', 'corebank_secure_password_2024')
            }
            
            print(f"æ­£åœ¨è¿æ¥æ•°æ®åº“: {db_config['host']}:{db_config['port']}/{db_config['database']}")
            
            self.connection = psycopg2.connect(**db_config)
            self.cursor = self.connection.cursor(cursor_factory=RealDictCursor)
            
            # æµ‹è¯•è¿æ¥
            self.cursor.execute("SELECT version();")
            version = self.cursor.fetchone()
            print(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {version['version']}")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def get_all_tables(self) -> List[str]:
        """è·å–æ‰€æœ‰ç”¨æˆ·è¡¨"""
        query = """
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_type = 'BASE TABLE'
        ORDER BY table_name;
        """
        
        self.cursor.execute(query)
        tables = [row['table_name'] for row in self.cursor.fetchall()]
        print(f"å‘ç° {len(tables)} ä¸ªè¡¨: {', '.join(tables)}")
        return tables
    
    def get_table_columns(self, table_name: str) -> List[ColumnInfo]:
        """è·å–è¡¨çš„åˆ—ä¿¡æ¯"""
        query = """
        SELECT 
            c.column_name,
            c.data_type,
            c.is_nullable = 'YES' as is_nullable,
            c.column_default,
            c.character_maximum_length,
            c.numeric_precision,
            c.numeric_scale,
            CASE WHEN pk.column_name IS NOT NULL THEN true ELSE false END as is_primary_key,
            CASE WHEN fk.column_name IS NOT NULL THEN true ELSE false END as is_foreign_key,
            fk.foreign_table_name as foreign_table,
            fk.foreign_column_name as foreign_column
        FROM information_schema.columns c
        LEFT JOIN (
            SELECT ku.column_name
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage ku 
                ON tc.constraint_name = ku.constraint_name
            WHERE tc.table_name = %s 
            AND tc.constraint_type = 'PRIMARY KEY'
        ) pk ON c.column_name = pk.column_name
        LEFT JOIN (
            SELECT 
                ku.column_name,
                ccu.table_name AS foreign_table_name,
                ccu.column_name AS foreign_column_name
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage ku 
                ON tc.constraint_name = ku.constraint_name
            JOIN information_schema.constraint_column_usage ccu 
                ON ccu.constraint_name = tc.constraint_name
            WHERE tc.table_name = %s 
            AND tc.constraint_type = 'FOREIGN KEY'
        ) fk ON c.column_name = fk.column_name
        WHERE c.table_name = %s
        ORDER BY c.ordinal_position;
        """
        
        self.cursor.execute(query, (table_name, table_name, table_name))
        columns = []
        
        for row in self.cursor.fetchall():
            column = ColumnInfo(
                column_name=row['column_name'],
                data_type=row['data_type'],
                is_nullable=row['is_nullable'],
                column_default=row['column_default'],
                character_maximum_length=row['character_maximum_length'],
                numeric_precision=row['numeric_precision'],
                numeric_scale=row['numeric_scale'],
                is_primary_key=row['is_primary_key'],
                is_foreign_key=row['is_foreign_key'],
                foreign_table=row['foreign_table'],
                foreign_column=row['foreign_column']
            )
            columns.append(column)
        
        return columns
    
    def get_table_indexes(self, table_name: str) -> List[IndexInfo]:
        """è·å–è¡¨çš„ç´¢å¼•ä¿¡æ¯"""
        query = """
        SELECT 
            i.indexname as index_name,
            i.tablename as table_name,
            array_agg(a.attname ORDER BY a.attnum) as column_names,
            ix.indisunique as is_unique,
            ix.indisprimary as is_primary,
            am.amname as index_type
        FROM pg_indexes i
        JOIN pg_class c ON c.relname = i.tablename
        JOIN pg_index ix ON ix.indexrelid = (
            SELECT oid FROM pg_class WHERE relname = i.indexname
        )
        JOIN pg_class ic ON ic.oid = ix.indexrelid
        JOIN pg_am am ON am.oid = ic.relam
        JOIN pg_attribute a ON a.attrelid = c.oid AND a.attnum = ANY(ix.indkey)
        WHERE i.tablename = %s
        AND i.schemaname = 'public'
        GROUP BY i.indexname, i.tablename, ix.indisunique, ix.indisprimary, am.amname
        ORDER BY i.indexname;
        """
        
        self.cursor.execute(query, (table_name,))
        indexes = []
        
        for row in self.cursor.fetchall():
            index = IndexInfo(
                index_name=row['index_name'],
                table_name=row['table_name'],
                column_names=row['column_names'],
                is_unique=row['is_unique'],
                is_primary=row['is_primary'],
                index_type=row['index_type']
            )
            indexes.append(index)
        
        return indexes
    
    def get_table_constraints(self, table_name: str) -> List[Dict[str, Any]]:
        """è·å–è¡¨çš„çº¦æŸä¿¡æ¯"""
        query = """
        SELECT 
            tc.constraint_name,
            tc.constraint_type,
            kcu.column_name,
            ccu.table_name AS foreign_table_name,
            ccu.column_name AS foreign_column_name,
            rc.match_option,
            rc.update_rule,
            rc.delete_rule
        FROM information_schema.table_constraints tc
        LEFT JOIN information_schema.key_column_usage kcu 
            ON tc.constraint_name = kcu.constraint_name
        LEFT JOIN information_schema.constraint_column_usage ccu 
            ON ccu.constraint_name = tc.constraint_name
        LEFT JOIN information_schema.referential_constraints rc 
            ON tc.constraint_name = rc.constraint_name
        WHERE tc.table_name = %s
        ORDER BY tc.constraint_type, tc.constraint_name;
        """
        
        self.cursor.execute(query, (table_name,))
        constraints = []
        
        for row in self.cursor.fetchall():
            constraint = {
                'constraint_name': row['constraint_name'],
                'constraint_type': row['constraint_type'],
                'column_name': row['column_name'],
                'foreign_table_name': row['foreign_table_name'],
                'foreign_column_name': row['foreign_column_name'],
                'match_option': row['match_option'],
                'update_rule': row['update_rule'],
                'delete_rule': row['delete_rule']
            }
            constraints.append(constraint)
        
        return constraints
    
    def get_table_stats(self, table_name: str) -> Dict[str, Any]:
        """è·å–è¡¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        # è·å–è¡Œæ•°
        self.cursor.execute(f"SELECT COUNT(*) as row_count FROM {table_name};")
        row_count = self.cursor.fetchone()['row_count']
        
        # è·å–è¡¨å¤§å°
        self.cursor.execute("""
            SELECT pg_size_pretty(pg_total_relation_size(%s)) as table_size;
        """, (table_name,))
        table_size = self.cursor.fetchone()['table_size']
        
        return {
            'row_count': row_count,
            'table_size': table_size
        }

    def analyze_table(self, table_name: str) -> TableInfo:
        """åˆ†æå•ä¸ªè¡¨çš„å®Œæ•´ä¿¡æ¯"""
        print(f"æ­£åœ¨åˆ†æè¡¨: {table_name}")

        columns = self.get_table_columns(table_name)
        indexes = self.get_table_indexes(table_name)
        constraints = self.get_table_constraints(table_name)
        stats = self.get_table_stats(table_name)

        table_info = TableInfo(
            table_name=table_name,
            columns=columns,
            indexes=indexes,
            row_count=stats['row_count'],
            table_size=stats['table_size'],
            constraints=constraints
        )

        return table_info

    def analyze_all_tables(self) -> Dict[str, TableInfo]:
        """åˆ†ææ‰€æœ‰è¡¨"""
        tables = self.get_all_tables()
        table_infos = {}

        for table_name in tables:
            try:
                table_info = self.analyze_table(table_name)
                table_infos[table_name] = table_info
            except Exception as e:
                print(f"åˆ†æè¡¨ {table_name} æ—¶å‡ºé”™: {e}")
                continue

        return table_infos

    def analyze_database_design(self, table_infos: Dict[str, TableInfo]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®åº“è®¾è®¡çš„åˆç†æ€§"""
        analysis = {
            'summary': {},
            'issues': [],
            'recommendations': [],
            'strengths': []
        }

        total_tables = len(table_infos)
        total_columns = sum(len(table.columns) for table in table_infos.values())
        total_indexes = sum(len(table.indexes) for table in table_infos.values())
        total_rows = sum(table.row_count for table in table_infos.values())

        analysis['summary'] = {
            'total_tables': total_tables,
            'total_columns': total_columns,
            'total_indexes': total_indexes,
            'total_rows': total_rows
        }

        # åˆ†ææ¯ä¸ªè¡¨
        for table_name, table_info in table_infos.items():
            self._analyze_table_design(table_name, table_info, analysis)

        # åˆ†æè¡¨é—´å…³ç³»
        self._analyze_relationships(table_infos, analysis)

        return analysis

    def _analyze_table_design(self, table_name: str, table_info: TableInfo, analysis: Dict[str, Any]):
        """åˆ†æå•ä¸ªè¡¨çš„è®¾è®¡"""

        # æ£€æŸ¥ä¸»é”®
        primary_keys = [col for col in table_info.columns if col.is_primary_key]
        if not primary_keys:
            analysis['issues'].append(f"è¡¨ {table_name} ç¼ºå°‘ä¸»é”®")
        elif len(primary_keys) > 1:
            analysis['strengths'].append(f"è¡¨ {table_name} ä½¿ç”¨å¤åˆä¸»é”®")

        # æ£€æŸ¥UUIDä¸»é”®
        uuid_pk = any(col.is_primary_key and col.data_type == 'uuid' for col in table_info.columns)
        if uuid_pk:
            analysis['strengths'].append(f"è¡¨ {table_name} ä½¿ç”¨UUIDä¸»é”®ï¼Œæœ‰åˆ©äºåˆ†å¸ƒå¼ç³»ç»Ÿ")

        # æ£€æŸ¥æ—¶é—´æˆ³å­—æ®µ
        timestamp_fields = [col for col in table_info.columns if 'timestamp' in col.data_type.lower()]
        if len(timestamp_fields) >= 2:
            analysis['strengths'].append(f"è¡¨ {table_name} åŒ…å«åˆ›å»ºå’Œæ›´æ–°æ—¶é—´æˆ³")
        elif len(timestamp_fields) == 1:
            analysis['recommendations'].append(f"è¡¨ {table_name} å»ºè®®æ·»åŠ æ›´æ–°æ—¶é—´æˆ³å­—æ®µ")
        else:
            analysis['issues'].append(f"è¡¨ {table_name} ç¼ºå°‘æ—¶é—´æˆ³å­—æ®µ")

        # æ£€æŸ¥ç´¢å¼•è¦†ç›–
        indexed_columns = set()
        for index in table_info.indexes:
            indexed_columns.update(index.column_names)

        foreign_key_columns = [col.column_name for col in table_info.columns if col.is_foreign_key]
        unindexed_fks = [fk for fk in foreign_key_columns if fk not in indexed_columns]

        if unindexed_fks:
            analysis['issues'].append(f"è¡¨ {table_name} çš„å¤–é”®å­—æ®µ {unindexed_fks} ç¼ºå°‘ç´¢å¼•")

        # æ£€æŸ¥æ•°æ®ç±»å‹åˆç†æ€§
        for col in table_info.columns:
            if col.data_type == 'text' and col.column_name in ['email', 'phone', 'code']:
                analysis['recommendations'].append(
                    f"è¡¨ {table_name} çš„ {col.column_name} å­—æ®µå»ºè®®ä½¿ç”¨VARCHARé™åˆ¶é•¿åº¦"
                )

            if col.data_type == 'numeric' and col.numeric_precision and col.numeric_precision > 19:
                analysis['issues'].append(
                    f"è¡¨ {table_name} çš„ {col.column_name} å­—æ®µç²¾åº¦è¿‡é«˜ï¼Œå¯èƒ½å½±å“æ€§èƒ½"
                )

        # æ£€æŸ¥è¡¨å¤§å°
        if table_info.row_count > 100000:
            analysis['recommendations'].append(f"è¡¨ {table_name} æ•°æ®é‡è¾ƒå¤§({table_info.row_count}è¡Œ)ï¼Œå»ºè®®è€ƒè™‘åˆ†åŒº")

    def _analyze_relationships(self, table_infos: Dict[str, TableInfo], analysis: Dict[str, Any]):
        """åˆ†æè¡¨é—´å…³ç³»"""

        # ç»Ÿè®¡å¤–é”®å…³ç³»
        foreign_keys = {}
        for table_name, table_info in table_infos.items():
            for col in table_info.columns:
                if col.is_foreign_key:
                    if col.foreign_table not in foreign_keys:
                        foreign_keys[col.foreign_table] = []
                    foreign_keys[col.foreign_table].append({
                        'from_table': table_name,
                        'from_column': col.column_name,
                        'to_column': col.foreign_column
                    })

        # æ£€æŸ¥å¼•ç”¨å®Œæ•´æ€§
        for referenced_table, references in foreign_keys.items():
            if referenced_table not in table_infos:
                analysis['issues'].append(f"å¤–é”®å¼•ç”¨çš„è¡¨ {referenced_table} ä¸å­˜åœ¨")
            else:
                analysis['strengths'].append(
                    f"è¡¨ {referenced_table} è¢« {len(references)} ä¸ªè¡¨å¼•ç”¨ï¼Œå…³ç³»è®¾è®¡è‰¯å¥½"
                )

        # æ£€æŸ¥å­¤ç«‹è¡¨
        referenced_tables = set(foreign_keys.keys())
        referencing_tables = set()
        for table_info in table_infos.values():
            if any(col.is_foreign_key for col in table_info.columns):
                referencing_tables.add(table_info.table_name)

        all_tables = set(table_infos.keys())
        isolated_tables = all_tables - referenced_tables - referencing_tables

        if isolated_tables:
            analysis['recommendations'].append(
                f"è¡¨ {list(isolated_tables)} æ²¡æœ‰å¤–é”®å…³ç³»ï¼Œè¯·ç¡®è®¤æ˜¯å¦éœ€è¦å»ºç«‹å…³è”"
            )

    def generate_report(self, table_infos: Dict[str, TableInfo], analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# æ•°è„‰é“¶è¡Œæ•°æ®åº“ç»“æ„åˆ†ææŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # æ¦‚è§ˆ
        report.append("## ğŸ“Š æ•°æ®åº“æ¦‚è§ˆ")
        summary = analysis['summary']
        report.append(f"- æ€»è¡¨æ•°: {summary['total_tables']}")
        report.append(f"- æ€»å­—æ®µæ•°: {summary['total_columns']}")
        report.append(f"- æ€»ç´¢å¼•æ•°: {summary['total_indexes']}")
        report.append(f"- æ€»è®°å½•æ•°: {summary['total_rows']:,}")
        report.append("")

        # è¡¨ç»“æ„è¯¦æƒ…
        report.append("## ğŸ“‹ è¡¨ç»“æ„è¯¦æƒ…")
        for table_name, table_info in sorted(table_infos.items()):
            report.append(f"### {table_name}")
            report.append(f"- è®°å½•æ•°: {table_info.row_count:,}")
            report.append(f"- å­˜å‚¨å¤§å°: {table_info.table_size}")
            report.append(f"- å­—æ®µæ•°: {len(table_info.columns)}")
            report.append(f"- ç´¢å¼•æ•°: {len(table_info.indexes)}")

            # å­—æ®µä¿¡æ¯
            report.append("\n#### å­—æ®µä¿¡æ¯")
            report.append("| å­—æ®µå | ç±»å‹ | å¯ç©º | é»˜è®¤å€¼ | ä¸»é”® | å¤–é”® |")
            report.append("|--------|------|------|--------|------|------|")

            for col in table_info.columns:
                nullable = "æ˜¯" if col.is_nullable else "å¦"
                pk = "âœ“" if col.is_primary_key else ""
                fk = f"â†’{col.foreign_table}.{col.foreign_column}" if col.is_foreign_key else ""
                default = col.column_default or ""
                if len(default) > 20:
                    default = default[:20] + "..."

                report.append(f"| {col.column_name} | {col.data_type} | {nullable} | {default} | {pk} | {fk} |")

            # ç´¢å¼•ä¿¡æ¯
            if table_info.indexes:
                report.append("\n#### ç´¢å¼•ä¿¡æ¯")
                report.append("| ç´¢å¼•å | å­—æ®µ | å”¯ä¸€ | ç±»å‹ |")
                report.append("|--------|------|------|------|")

                for idx in table_info.indexes:
                    unique = "âœ“" if idx.is_unique else ""
                    columns = ", ".join(idx.column_names)
                    report.append(f"| {idx.index_name} | {columns} | {unique} | {idx.index_type} |")

            report.append("")

        # è®¾è®¡åˆ†æ
        report.append("## ğŸ” è®¾è®¡åˆ†æ")

        if analysis['strengths']:
            report.append("### âœ… è®¾è®¡ä¼˜ç‚¹")
            for strength in analysis['strengths']:
                report.append(f"- {strength}")
            report.append("")

        if analysis['issues']:
            report.append("### âš ï¸ å‘ç°é—®é¢˜")
            for issue in analysis['issues']:
                report.append(f"- {issue}")
            report.append("")

        if analysis['recommendations']:
            report.append("### ğŸ’¡ ä¼˜åŒ–å»ºè®®")
            for rec in analysis['recommendations']:
                report.append(f"- {rec}")
            report.append("")

        return "\n".join(report)

    def save_results(self, table_infos: Dict[str, TableInfo], analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_data = {
            'timestamp': timestamp,
            'tables': {name: asdict(info) for name, info in table_infos.items()},
            'analysis': analysis
        }

        json_file = f"database_structure_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)

        # ä¿å­˜Markdownæ ¼å¼çš„æŠ¥å‘Š
        report = self.generate_report(table_infos, analysis)
        report_file = f"database_analysis_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"åˆ†æç»“æœå·²ä¿å­˜:")
        print(f"- è¯¦ç»†æ•°æ®: {json_file}")
        print(f"- åˆ†ææŠ¥å‘Š: {report_file}")

    def close_connection(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.cursor:
            self.cursor.close()
        if self.connection:
            self.connection.close()
        print("æ•°æ®åº“è¿æ¥å·²å…³é—­")


def load_env_file(env_path: str = "corebank-backend/.env"):
    """åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡"""
    if not os.path.exists(env_path):
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡æ–‡ä»¶ {env_path}")
        return

    with open(env_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦ æ•°è„‰é“¶è¡Œæ•°æ®åº“ç»“æ„åˆ†æå·¥å…·")
    print("=" * 50)

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env_file()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = DatabaseAnalyzer()

    try:
        # è¿æ¥æ•°æ®åº“
        if not analyzer.connect_to_database():
            return 1

        # åˆ†ææ‰€æœ‰è¡¨
        print("\nå¼€å§‹åˆ†ææ•°æ®åº“ç»“æ„...")
        table_infos = analyzer.analyze_all_tables()

        if not table_infos:
            print("æœªæ‰¾åˆ°ä»»ä½•è¡¨")
            return 1

        # åˆ†æè®¾è®¡åˆç†æ€§
        print("\nå¼€å§‹åˆ†æè®¾è®¡åˆç†æ€§...")
        analysis = analyzer.analyze_database_design(table_infos)

        # ä¿å­˜ç»“æœ
        analyzer.save_results(table_infos, analysis)

        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        print(f"\nğŸ“Š åˆ†æå®Œæˆ!")
        print(f"- åˆ†æäº† {len(table_infos)} ä¸ªè¡¨")
        print(f"- å‘ç° {len(analysis['issues'])} ä¸ªé—®é¢˜")
        print(f"- æå‡º {len(analysis['recommendations'])} ä¸ªå»ºè®®")
        print(f"- è¯†åˆ« {len(analysis['strengths'])} ä¸ªä¼˜ç‚¹")

        return 0

    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return 1

    finally:
        analyzer.close_connection()


if __name__ == "__main__":
    sys.exit(main())
